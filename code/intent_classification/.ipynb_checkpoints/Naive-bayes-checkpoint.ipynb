{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\MathildeElimas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MathildeElimas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\MathildeElimas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "from data_processing import *\n",
    "\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.model_selection import train_test_split, cross_validate, KFold\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getcwd() + '\\\\data'\n",
    "method = 'tfidf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Sentence           Intent\n",
      "0  Bonjour j'ai rendu mon équipement box est je n...  probleme_box_tv\n",
      "1                                            Bonjour  probleme_box_tv\n",
      "2                              Box internet en panne  probleme_box_tv\n",
      "3  Plus de phone ni internet depuis plus ou moins...  probleme_box_tv\n",
      "4  Bonjour j'ai la télécommande de ma box qui ne ...  probleme_box_tv\n"
     ]
    }
   ],
   "source": [
    "# intent, unique_intent, sentences = load_data(PATH + '\\\\training_recast2.csv', \n",
    "#                                              sep = ';', header = 1, encoding = \"cp1252\", \n",
    "#                                              names = [\"Sentence\", \"Intent\", \"Description\"], index_col= False)\n",
    "\n",
    "intent, unique_intent, sentences = load_data(PATH + '\\\\intent_dialogues.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create vocabulary & output labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-25 19:04:50,852 - spacy_lefff.lefff - INFO - New LefffLemmatizer instantiated.\n",
      "2020-10-25 19:04:50,854 - spacy_lefff.lefff - INFO - Reading lefff data...\n",
      "2020-10-25 19:04:51,957 - spacy_lefff.lefff - INFO - Successfully loaded lefff lemmatizer\n",
      "Vocab Size = 1684 and Maximum length = 291\n"
     ]
    }
   ],
   "source": [
    "idx, cleaned_words = clean_sentences(sentences)\n",
    "word_tokenizer = create_tokenizer(cleaned_words)\n",
    "\n",
    "vocab_size = len(word_tokenizer.word_index) + 1\n",
    "max_length = len(max(cleaned_words, key = len))\n",
    "\n",
    "print(\"Vocab Size = %d and Maximum length = %d\" % (vocab_size, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-25 19:06:05,811 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = create_vocabulary(cleaned_words)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'explication_facture': 1, 'probleme_box_tv': 2, 'reseau_mobile': 3}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tokenizer = create_tokenizer(unique_intent, filters = '')\n",
    "output_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent = list(map(intent.__getitem__, idx))\n",
    "\n",
    "encoded_output = intent_embedding(intent)\n",
    "encoded_output = np.array(encoded_output) #.reshape(len(encoded_output), 1)\n",
    "\n",
    "encoded_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert document to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padded_doc = padding_doc(encoded_doc, max_length)\n",
    "# print(\"Shape of padded docs = \",padded_doc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = [output_tokenizer.word_index[intent.lower()] for intent in intent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Count_vect' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-e03228032caa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'count'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mCount_vect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mCount_vect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleaned_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCount_vect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleaned_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Count_vect' is not defined"
     ]
    }
   ],
   "source": [
    "if method == 'tfidf':\n",
    "    Tfidf_vect = TfidfVectorizer(max_features=5000, vocabulary = list(vocab.keys()))\n",
    "    Tfidf_vect.fit(cleaned_words)\n",
    "\n",
    "    X = Tfidf_vect.transform(cleaned_words)\n",
    "elif method == 'count':\n",
    "    Count_vect = CountVectorizer(max_features=5000, vocabulary = list(vocab.keys()))\n",
    "    Count_vect.fit(cleaned_words)\n",
    "\n",
    "    X = Count_vect.transform(cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X, Test_X, Train_Y, Test_Y = train_test_split(X, Y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model definition & training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = KFold(n_splits=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the NB classifier\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "\n",
    "cv = cross_validate(Naive, X, Y, cv = k_fold, scoring = 'balanced_accuracy')\n",
    "Naive.fit(Train_X,Train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.getcwd() + '/resultats/naiveBayes_' + method +'.pkl') as f:\n",
    "    pickle.dump(boost,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv['test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the labels on validation dataset\n",
    "predictions_NB = Naive.predict(Test_X)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Naive Bayes Accuracy Score -> \",balanced_accuracy_score(Test_Y, predictions_NB)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Je ne reçois pas internet sur mon téléphone.\"\n",
    "# pred = int(Naive.predict(Tfidf_vect.transform([text])))\n",
    "pred = int(Naive.predict(Count_vect.transform([text])))\n",
    "output_tokenizer.index_word[pred]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
